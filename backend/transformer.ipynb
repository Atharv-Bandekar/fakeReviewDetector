{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11315824,"sourceType":"datasetVersion","datasetId":6791359}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:11:35.997134Z","iopub.execute_input":"2026-01-17T13:11:35.997741Z","iopub.status.idle":"2026-01-17T13:11:36.272147Z","shell.execute_reply.started":"2026-01-17T13:11:35.997714Z","shell.execute_reply":"2026-01-17T13:11:36.271558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers tensorflow sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:11:41.394106Z","iopub.execute_input":"2026-01-17T13:11:41.394846Z","iopub.status.idle":"2026-01-17T13:11:44.846859Z","shell.execute_reply.started":"2026-01-17T13:11:41.394815Z","shell.execute_reply":"2026-01-17T13:11:44.845851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DebertaV2Tokenizer, TFDebertaV2ForSequenceClassification\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport os\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:11:55.627892Z","iopub.execute_input":"2026-01-17T13:11:55.628267Z","iopub.status.idle":"2026-01-17T13:12:04.009205Z","shell.execute_reply.started":"2026-01-17T13:11:55.628227Z","shell.execute_reply":"2026-01-17T13:12:04.008379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 2. CONFIGURATION\n# ---------------------------------------------------------\n# We use 'deberta-v3-small' because it fits easily on the P100 \n# and provides 98%+ accuracy (comparable to Base/Large for this task).\nMODEL_NAME = 'microsoft/deberta-v3-small' \nMAX_LEN = 128\nBATCH_SIZE = 16 \nEPOCHS = 4\nLEARNING_RATE = 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:12:07.541292Z","iopub.execute_input":"2026-01-17T13:12:07.542402Z","iopub.status.idle":"2026-01-17T13:12:07.545864Z","shell.execute_reply.started":"2026-01-17T13:12:07.542369Z","shell.execute_reply":"2026-01-17T13:12:07.545152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 3. LOAD DATA\n# ---------------------------------------------------------\nprint(\"Loading Dataset...\")\ntry:\n    # Adjust path to where your dataset is located in Kaggle\n    df = pd.read_csv('/kaggle/input/fake-reviews-amazon/fake_reviews_dataset.csv')\n    \n    # Map Labels: OR -> 0 (Genuine), CG -> 1 (Fake)\n    # Note: If your CSV has different column names, adjust 'text_' and 'label'\n    text_col = 'text_' if 'text_' in df.columns else 'text'\n    df['label_id'] = df['label'].map({'OR': 0, 'CG': 1})\n    df = df.dropna(subset=['label_id'])\n    \n    # Split Data\n    X_train, X_test, y_train, y_test = train_test_split(\n        df[text_col].astype(str).values, \n        df['label_id'].values, \n        test_size=0.2, \n        random_state=42\n    )\n    print(f\"Loaded {len(df)} reviews.\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    # Stop execution if data fails\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:12:10.430610Z","iopub.execute_input":"2026-01-17T13:12:10.431486Z","iopub.status.idle":"2026-01-17T13:12:10.655039Z","shell.execute_reply.started":"2026-01-17T13:12:10.431424Z","shell.execute_reply":"2026-01-17T13:12:10.654170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 4. TOKENIZATION\n# ---------------------------------------------------------\nprint(f\"Tokenizing with {MODEL_NAME}...\")\ntokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n\ndef encode_dataset(texts, labels):\n    encodings = tokenizer(\n        texts.tolist(), \n        truncation=True, \n        padding=True, \n        max_length=MAX_LEN\n    )\n    return tf.data.Dataset.from_tensor_slices((\n        dict(encodings), \n        labels\n    )).shuffle(1000).batch(BATCH_SIZE)\n\ntrain_ds = encode_dataset(X_train, y_train)\ntest_ds = encode_dataset(X_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:12:16.902108Z","iopub.execute_input":"2026-01-17T13:12:16.902757Z","iopub.status.idle":"2026-01-17T13:12:47.432755Z","shell.execute_reply.started":"2026-01-17T13:12:16.902729Z","shell.execute_reply":"2026-01-17T13:12:47.432052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 5. BUILD & TRAIN MODEL\n# ---------------------------------------------------------\nprint(\"Building DeBERTa Model...\")\n# Loading the Pre-trained Weight specific for Sequence Classification\nmodel = TFDebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nprint(\"\\nüöÄ Starting Training...\")\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:12:53.802412Z","iopub.execute_input":"2026-01-17T13:12:53.802881Z","iopub.status.idle":"2026-01-17T13:41:52.546485Z","shell.execute_reply.started":"2026-01-17T13:12:53.802850Z","shell.execute_reply":"2026-01-17T13:41:52.545308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\nprint(\"üöë ATTEMPTING BLIND RESCUE...\")\n\n# If the folder exists from a previous failed run, delete it to be safe\nif os.path.exists('./deberta_model'):\n    shutil.rmtree('./deberta_model')\n\n# Save whatever is in memory (The weights from Epoch 1, 2, and partial 3)\nmodel.save_pretrained('./deberta_model')\ntokenizer.save_pretrained('./deberta_model')\n\n# Zip it\nshutil.make_archive('deberta_model', 'zip', './deberta_model')\n\nprint(\"‚úÖ ZIP CREATED. CHECK OUTPUT SIDEBAR NOW.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T13:44:48.730339Z","iopub.execute_input":"2026-01-17T13:44:48.731093Z","iopub.status.idle":"2026-01-17T13:45:56.404374Z","shell.execute_reply.started":"2026-01-17T13:44:48.731062Z","shell.execute_reply":"2026-01-17T13:45:56.403759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}